{"cells":[{"cell_type":"code","source":["from __future__ import print_function\nimport random as rand\nimport csv\nimport operator\nimport gc\nimport os\nfrom datetime import datetime\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import load_model\nimport keras.backend as K\nfrom sklearn.metrics import log_loss\nimport numpy as np\nimport mlflow\nimport mlflow.keras\n\nif K.backend() == 'tensorflow':\n    import tensorflow as tf\nfrom setuptools import setup, find_packages\n\nfrom keras.datasets import mnist\nfrom keras.utils.np_utils import to_categorical\nfrom keras import backend as K\nfrom devol import DEvol, GenomeHandler\nimport pandas as pd\nfrom pyspark.sql.types import *\nimport inspect\nfrom typing import Callable, List\nimport pandas as pd\nfrom pyspark.sql import DataFrame, Row, column\nfrom pyspark.sql.functions import lit, pandas_udf, PandasUDFType, array\nfrom pyspark.sql.types import FloatType"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc46fb1f-06a7-4639-b0fd-3c735d5232b6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pickle \npickle_file = open('/dbfs/FileStore/models/simple/pickle','rb')\nretrieved_devol = pickle.load(pickle_file)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d016ffbb-9cc3-4c58-bfac-0e9b1b5729fe"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#use rdd to parallelize training\ndef transform_row(row):\n  model = None\n  loss = None\n  accurcacy = None\n  accuracy = 0\n  gene = row.v\n  gene = gene.split(sep = ',')\n  result = []\n  for item in gene:\n    result.append(int(item))\n    \n  if(genome_handler.is_compatible_genome(result)):\n    model, loss, accuracy = retrieved_devol._evaluate(result,1)\n  return model, loss, accuracy\nlam = lambda row: transform_row(row)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba88812a-6094-4959-85ec-73a7c98aac91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["if(len(tf.config.experimental.list_physical_devices('GPU')) > 0):\n  print(\"GPU accelerated\")\n  print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nelse:\n  print(\"CPU mode\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"75a4096c-033c-470a-bc0c-36f2752cc845"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">GPU accelerated\nNum GPUs Available:  2\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">GPU accelerated\nNum GPUs Available:  2\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["\"\"\"\nRun a genetic algorithm to find an appropriate architecture for some image\nclassification task with Keras+TF.\nTo use, define a `GenomeHandler` defined in genomehandler.py. Then pass it, with\ntraining data, to a DEvol instance to run the genetic algorithm. See the readme\nfor more detailed instructions.\n\"\"\"\n\nfrom __future__ import print_function\nimport random as rand\nimport csv\nimport operator\nimport gc\nimport os\nfrom datetime import datetime\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import load_model\nimport keras.backend as K\nfrom sklearn.metrics import log_loss\nimport numpy as np\n\nif K.backend() == 'tensorflow':\n    import tensorflow as tf\n\n__all__ = ['DEvol']\n\nMETRIC_OPS = [operator.__lt__, operator.__gt__]\nMETRIC_OBJECTIVES = [min, max]\n\ndbutils.fs.mkdirs( '/dbfs/FileStore/models/gpu/')\n\n\nclass DEvol:\n    \"\"\"\n    Object which carries out genetic search and returns top performing model\n    upon completion.\n    \"\"\"\n#!!! data_path with checkpointing\n    def __init__(self, genome_handler, data_path=\"/dbfs/FileStore/runs/\"):\n        \"\"\"\n        Initialize a DEvol object which carries out the training and evaluation\n        of a genetic search.\n        Args:\n            genome_handler (GenomeHandler): the genome handler object defining\n                    the restrictions for the architecture search space\n            data_path (str): the file which the genome encodings and metric data\n                    will be stored in\n        \"\"\"\n        self.data_path = data_path + datetime.now().ctime().replace(\":\",\"-\").replace(\" \",\"_\")\n        data_path = self.data_path\n        dbutils.fs.mkdirs(data_path )\n        \n        self.genome_handler = genome_handler\n        self.datafile = data_path or (data_path + 'record.csv')\n        self._bssf = -1\n\n\n\n        if os.path.isfile(data_path) and os.stat(data_path).st_size > 1:\n            raise ValueError(('Non-empty file %s already exists. Please change'\n                              'file path to prevent overwritten genome data.'\n                              % data_path))\n\n        print(\"Genome encoding and metric data stored at\", self.datafile, \"\\n\")\n        with open(self.datafile, 'a') as csvfile:\n            writer = csv.writer(csvfile, delimiter=',', quotechar='\"',\n                                quoting=csv.QUOTE_MINIMAL)\n            metric_cols = [\"Val Loss\", \"Val Accuracy\"]\n            genome = genome_handler.genome_representation() + metric_cols\n            writer.writerow(genome)\n\n    def set_objective(self, metric):\n        \"\"\"\n        Set the metric for optimization. Can also be done by passing to\n        `run`.\n        Args:\n            metric (str): either 'acc' to maximize classification accuracy, or\n                    else 'loss' to minimize the loss function\n        \"\"\"\n        if metric == 'acc':\n            metric = 'accuracy'\n        if metric not in ['loss', 'accuracy']:\n            raise ValueError(('Invalid metric name {} provided - should be'\n                              '\"accuracy\" or \"loss\"').format(metric))\n        self._metric = metric\n        self._objective = \"max\" if self._metric == \"accuracy\" else \"min\"\n        self._metric_index = 1 if self._metric == 'loss' else -1\n        self._metric_op = METRIC_OPS[self._objective == 'max']\n        self._metric_objective = METRIC_OBJECTIVES[self._objective == 'max']\n       \n        \n    def run(self, dataset, num_generations, pop_size, epochs, fitness=None,\n            metric='accuracy'):\n        \"\"\"\n        Run genetic search on dataset given number of generations and\n        population size\n        Args:\n            dataset : tuple or list of numpy arrays in form ((train_data,\n                    train_labels), (validation_data, validation_labels))\n            num_generations (int): number of generations to search\n            pop_size (int): initial population size\n            epochs (int): epochs for each model eval, passed to keras model.fit\n            fitness (None, optional): scoring function to be applied to\n                    population scores, will be called on a numpy array which is\n                    a min/max scaled version of evaluated model metrics, so It\n                    should accept a real number including 0. If left as default\n                    just the min/max scaled values will be used.\n            metric (str, optional): must be \"accuracy\" or \"loss\" , defines what\n                    to optimize during search\n        Returns:\n            keras model: best model found with weights\n        \"\"\"\n        self.set_objective(metric)\n\n        # If no validation data is given set it to None\n        if len(dataset) == 2:\n            (self.x_train, self.y_train), (self.x_test, self.y_test) = dataset\n            self.x_val = None\n            self.y_val = None\n        else:\n            (self.x_train, self.y_train), (self.x_test, self.y_test), (self.x_val, self.y_val) = dataset\n\n        # generate and evaluate initial population\n        members = self._generate_random_population(pop_size)\n        pop = self._evaluate_population(members,\n                                        epochs,\n                                        fitness,\n                                        0,\n                                        num_generations)\n\n        # evolve\n        for gen in range(1, num_generations):\n            members = self._reproduce(pop, gen)\n            #!!! map to pandas df, apply udf parallel training, save scores\n            pop = self._evaluate_population(members,\n                                            epochs,\n                                            fitness,\n                                            gen,\n                                            num_generations)\n        \n        #!!!add checkpointing to dbfs\n\n        return 1\n        #return load_model('best-model.h5')\n\n    def _reproduce(self, pop, gen):\n        members = []\n\n        # 95% from crossover\n        for _ in range(int(len(pop) * 0.95)):\n            members.append(self._crossover(pop.select(), pop.select()))\n\n        # best models survive automatically\n        members += pop.get_best(len(pop) - int(len(pop) * 0.95))\n\n        # randomly mutate\n        for imem, mem in enumerate(members):\n            members[imem] = self._mutate(mem, gen)\n        return members\n\n    def _evaluate(self, genome, epochs):\n        model = self.genome_handler.decode(genome)\n        loss, accuracy = None, None\n        fit_params = {\n            'x': self.x_train,\n            'y': self.y_train,\n            'validation_split': 0.1,\n            'epochs': epochs,\n            'verbose': 1,\n            'callbacks': [\n                EarlyStopping(monitor='val_loss', patience=1, verbose=1)\n            ]\n        }\n\n        if self.x_val is not None:\n            fit_params['validation_data'] = (self.x_val, self.y_val)\n        try:\n            model.fit(**fit_params)\n            loss, accuracy = model.evaluate(self.x_test, self.y_test, verbose=0)\n        except Exception as e:\n            loss, accuracy = self._handle_broken_model(model, e)\n\n        self._record_stats(model, genome, loss, accuracy)\n\n        return model, loss, accuracy\n\n    def _record_stats(self, model, genome, loss, accuracy):\n        with open(self.datafile, 'a') as csvfile:\n            writer = csv.writer(csvfile, delimiter=',',\n                                quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n            row = list(genome) + [loss, accuracy]\n            writer.writerow(row)\n\n        met = loss if self._metric == 'loss' else accuracy\n        if (self._bssf is -1 or\n                self._metric_op(met, self._bssf) and\n                accuracy is not 0):\n            \n            self._bssf = met\n            #azure enforces mlflow saving\n            mlflow.keras.save_model(model, self.data_path + '/best-model-mlflow')\n\n    def _handle_broken_model(self, model, error):\n        del model\n\n        n = self.genome_handler.n_classes\n        loss = log_loss(np.concatenate(([1], np.zeros(n - 1))), np.ones(n) / n)\n        accuracy = 1 / n\n        gc.collect()\n\n        if K.backend() == 'tensorflow':\n            K.clear_session()\n            tf.reset_default_graph()\n\n        print('An error occurred and the model could not train:')\n        print(error)\n        print(('Model assigned poor score. Please ensure that your model'\n               'constraints live within your computational resources.'))\n        return loss, accuracy\n\n    def  p_eval(self, df1, epochs):\n      #use rdd to parallelize training\n      \n      lam = lambda row: transform_row(row)\n\n      loss_acc_rdd = df1.rdd.map(lam).collect()\n      return loss_acc_rdd\n    \n    def _evaluate_population(self, members, epochs, fitness, igen, ngen):\n        df1 = pd.DataFrame()\n        list= []\n        for member in members:\n            gene = (str(member))[1:-1] \n            sep = ''\n            foo=(sep.join(gene))\n            list.append(foo)\n            print(foo)\n\n        df1 = pd.DataFrame(list)\n        df1 = spark.createDataFrame(df1, [ \"v\"])\n        \n        result = self.p_eval(df1, epochs)\n        \n        \n        fit = []\n        #data_path = \"/dbfs/FileStore/test/\"\n        #result2\n        for imem, mem in enumerate(members):\n          model = result[imem][0]\n          loss = result[imem][1]\n          accuracy = result[imem][2]\n          name = datetime.now().ctime() + \"_\" + str(accuracy) + \"_\" +str(loss)\n          \n          mlflow.keras.save_model(model,(self.data_path+ name))\n          #model.save_model(data_path + name)\n          print(\"saved :\",name,\"\\n\")\n          res = model, loss, accuracy\n          v = res[self._metric_index]\n          print(res[self._metric_index])\n          del res\n          fit.append(v)\n        \n        fit = np.array(fit)\n        self._print_result(fit, igen)\n        return _Population(members, fit, fitness, obj=self._objective)\n\n    def _print_evaluation(self, imod, nmod, igen, ngen):\n        fstr = '\\nmodel {0}/{1} - generation {2}/{3}:\\n'\n        print(fstr.format(imod + 1, nmod, igen + 1, ngen))\n\n    def _generate_random_population(self, size):\n        return [self.genome_handler.generate() for _ in range(size)]\n\n    def _print_result(self, fitness, generation):\n        result_str = ('Generation {3}:\\t\\tbest {4}: {0:0.4f}\\t\\taverage:'\n                      '{1:0.4f}\\t\\tstd: {2:0.4f}')\n        print(result_str.format(self._metric_objective(fitness),\n                                np.mean(fitness),\n                                np.std(fitness),\n                                generation + 1, self._metric))\n\n    def _crossover(self, genome1, genome2):\n        cross_ind = rand.randint(0, len(genome1))\n        child = genome1[:cross_ind] + genome2[cross_ind:]\n        return child\n\n    def _uniform_crossover(self,genome1,genome2):\n        prob_xover = 0.5\n        for i in range(len(genome1)):\n          if rand.random() < prob_xover:\n            genome1[i], genome2[i] = genome2[i], genome1[i]\n        return genome1, genome2\n      \n    def _mutate(self, genome, generation):\n        # increase mutations as program continues\n        num_mutations = max(3, generation // 4)\n        return self.genome_handler.mutate(genome, num_mutations)\n\n\nclass _Population(object):\n\n    def __len__(self):\n        return len(self.members)\n\n    def __init__(self, members, fitnesses, score, obj='max'):\n        self.members = members\n        scores = fitnesses - fitnesses.min()\n        if scores.max() > 0:\n            scores /= scores.max()\n        if obj == 'min':\n            scores = 1 - scores\n        if score:\n            self.scores = score(scores)\n        else:\n            self.scores = scores\n        self.s_fit = sum(self.scores)\n\n    def get_best(self, n):\n        combined = [(self.members[i], self.scores[i])\n                    for i in range(len(self.members))]\n        sorted(combined, key=(lambda x: x[1]), reverse=True)\n        return [x[0] for x in combined[:n]]\n\n    \n      \n    #fitness proportional selection\n    def select(self):\n        dart = rand.uniform(0, self.s_fit)\n        sum_fits = 0\n        for i in range(len(self.members)):\n            sum_fits += self.scores[i]\n            if sum_fits >= dart:\n                return self.members[i]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9644b11b-46aa-434d-80a2-a14addc2a2ec"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["import numpy as np\nimport random as rand\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\n\n\nclass GenomeHandler:\n    \"\"\"\n    Defines the configuration and handles the conversion and mutation of\n    individual genomes. Should be created and passed to a `DEvol` instance.\n    ---\n    Genomes are represented as fixed-with lists of integers corresponding\n    to sequential layers and properties. A model with 2 convolutional layers\n    and 1 dense layer would look like:\n    [<conv layer><conv layer><dense layer><optimizer>]\n    The makeup of the convolutional layers and dense layers is defined in the\n    GenomeHandler below under self.convolutional_layer_shape and\n    self.dense_layer_shape. <optimizer> consists of just one property.\n    \"\"\"\n\n    def __init__(self, max_conv_layers, max_dense_layers, max_filters,\n                 max_dense_nodes, input_shape, n_classes,\n                 batch_normalization=True, dropout=True, max_pooling=True,\n                 optimizers=None, activations=None):\n        \"\"\"\n        Creates a GenomeHandler according\n        Args:\n            max_conv_layers: The maximum number of convolutional layers\n            max_dense_layers: The maximum number of dense (fully connected)\n                    layers, including output layer\n            max_filters: The maximum number of conv filters (feature maps) in a\n                    convolutional layer\n            max_dense_nodes: The maximum number of nodes in a dense layer\n            input_shape: The shape of the input\n            n_classes: The number of classes\n            batch_normalization (bool): whether the GP should include batch norm\n            dropout (bool): whether the GP should include dropout\n            max_pooling (bool): whether the GP should include max pooling layers\n            optimizers (list): list of optimizers to be tried by the GP. By\n                    default, the network uses Keras's built-in adam, rmsprop,\n                    adagrad, and adadelta\n            activations (list): list of activation functions to be tried by the\n                    GP. By default, relu and sigmoid.\n        \"\"\"\n        if max_dense_layers < 1:\n            raise ValueError(\n                \"At least one dense layer is required for softmax layer\"\n            )\n        if max_filters > 0:\n            filter_range_max = int(math.log(max_filters, 2)) + 1\n        else:\n            filter_range_max = 0\n        self.optimizer = optimizers or [\n            'adam',\n            'rmsprop',\n            'adagrad',\n            'adadelta'\n        ]\n        self.activation = activations or [\n            'relu',\n            'sigmoid',\n        ]\n        self.convolutional_layer_shape = [\n            \"active\",\n            \"num filters\",\n            \"batch normalization\",\n            \"activation\",\n            \"dropout\",\n            \"max pooling\",\n        ]\n        self.dense_layer_shape = [\n            \"active\",\n            \"num nodes\",\n            \"batch normalization\",\n            \"activation\",\n            \"dropout\",\n        ]\n        self.layer_params = {\n            \"active\": [0, 1],\n            \"num filters\": [2**i for i in range(3, filter_range_max)],\n            \"num nodes\": [2**i for i in range(4, int(math.log(max_dense_nodes, 2)) + 1)],\n            \"batch normalization\": [0, (1 if batch_normalization else 0)],\n            \"activation\": list(range(len(self.activation))),\n            \"dropout\": [(i if dropout else 0) for i in range(11)],\n            \"max pooling\": list(range(3)) if max_pooling else 0,\n        }\n\n        self.convolution_layers = max_conv_layers\n        self.convolution_layer_size = len(self.convolutional_layer_shape)\n        self.dense_layers = max_dense_layers - 1  # this doesn't include the softmax layer, so -1\n        self.dense_layer_size = len(self.dense_layer_shape)\n        self.input_shape = input_shape\n        self.n_classes = n_classes\n\n    def convParam(self, i):\n        key = self.convolutional_layer_shape[i]\n        return self.layer_params[key]\n\n    def denseParam(self, i):\n        key = self.dense_layer_shape[i]\n        return self.layer_params[key]\n\n    def mutate(self, genome, num_mutations):\n        num_mutations = np.random.choice(num_mutations)\n        for i in range(num_mutations):\n            index = np.random.choice(list(range(1, len(genome))))\n            if index < self.convolution_layer_size * self.convolution_layers:\n                if genome[index - index % self.convolution_layer_size]:\n                    range_index = index % self.convolution_layer_size\n                    choice_range = self.convParam(range_index)\n                    genome[index] = np.random.choice(choice_range)\n                elif rand.uniform(0, 1) <= 0.01:  # randomly flip deactivated layers\n                    genome[index - index % self.convolution_layer_size] = 1\n            elif index != len(genome) - 1:\n                offset = self.convolution_layer_size * self.convolution_layers\n                new_index = (index - offset)\n                present_index = new_index - new_index % self.dense_layer_size\n                if genome[present_index + offset]:\n                    range_index = new_index % self.dense_layer_size\n                    choice_range = self.denseParam(range_index)\n                    genome[index] = np.random.choice(choice_range)\n                elif rand.uniform(0, 1) <= 0.01:\n                    genome[present_index + offset] = 1\n            else:\n                genome[index] = np.random.choice(list(range(len(self.optimizer))))\n        return genome\n\n    def decode(self, genome):\n        if not self.is_compatible_genome(genome):\n            raise ValueError(\"Invalid genome for specified configs\")\n        model = Sequential()\n        dim = 0\n        offset = 0\n        if self.convolution_layers > 0:\n            dim = min(self.input_shape[:-1])  # keep track of smallest dimension\n        input_layer = True\n        for i in range(self.convolution_layers):\n            if genome[offset]:\n                convolution = None\n                if input_layer:\n                    convolution = Convolution2D(\n                        genome[offset + 1], (3, 3),\n                        padding='same',\n                        input_shape=self.input_shape\n                    )\n                    input_layer = False\n                else:\n                    convolution = Convolution2D(\n                        genome[offset + 1], (3, 3),\n                        padding='same'\n                    )\n                model.add(convolution)\n                if genome[offset + 2]:\n                    model.add(BatchNormalization())\n                model.add(Activation(self.activation[genome[offset + 3]]))\n                model.add(Dropout(float(genome[offset + 4] / 20.0)))\n                max_pooling_type = genome[offset + 5]\n                # must be large enough for a convolution\n                if max_pooling_type == 1 and dim >= 5:\n                    model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n                    dim = int(math.ceil(dim / 2))\n            offset += self.convolution_layer_size\n\n        if not input_layer:\n            model.add(Flatten())\n\n        for i in range(self.dense_layers):\n            if genome[offset]:\n                dense = None\n                if input_layer:\n                    dense = Dense(genome[offset + 1], input_shape=self.input_shape)\n                    input_layer = False\n                else:\n                    dense = Dense(genome[offset + 1])\n                model.add(dense)\n                if genome[offset + 2]:\n                    model.add(BatchNormalization())\n                model.add(Activation(self.activation[genome[offset + 3]]))\n                model.add(Dropout(float(genome[offset + 4] / 20.0)))\n            offset += self.dense_layer_size\n\n        model.add(Dense(self.n_classes, activation='softmax'))\n        model.compile(loss='categorical_crossentropy',\n                      optimizer=self.optimizer[genome[offset]],\n                      metrics=[\"accuracy\"])\n        return model\n\n    def genome_representation(self):\n        encoding = []\n        for i in range(self.convolution_layers):\n            for key in self.convolutional_layer_shape:\n                encoding.append(\"Conv\" + str(i) + \" \" + key)\n        for i in range(self.dense_layers):\n            for key in self.dense_layer_shape:\n                encoding.append(\"Dense\" + str(i) + \" \" + key)\n        encoding.append(\"Optimizer\")\n        return encoding\n\n    def generate(self):\n        genome = []\n        for i in range(self.convolution_layers):\n            for key in self.convolutional_layer_shape:\n                param = self.layer_params[key]\n                genome.append(np.random.choice(param))\n        for i in range(self.dense_layers):\n            for key in self.dense_layer_shape:\n                param = self.layer_params[key]\n                genome.append(np.random.choice(param))\n        genome.append(np.random.choice(list(range(len(self.optimizer)))))\n        genome[0] = 1\n        return genome\n\n    def is_compatible_genome(self, genome):\n        expected_len = self.convolution_layers * self.convolution_layer_size \\\n            + self.dense_layers * self.dense_layer_size + 1\n        if len(genome) != expected_len:\n            return False\n        ind = 0\n        for i in range(self.convolution_layers):\n            for j in range(self.convolution_layer_size):\n                if genome[ind + j] not in self.convParam(j):\n                    return False\n            ind += self.convolution_layer_size\n        for i in range(self.dense_layers):\n            for j in range(self.dense_layer_size):\n                if genome[ind + j] not in self.denseParam(j):\n                    return False\n            ind += self.dense_layer_size\n        if genome[ind] not in range(len(self.optimizer)):\n            return False\n        return True\n\n    def best_genome(self, csv_path, metric=\"accuracy\", include_metrics=True):\n        best = max if metric is \"accuracy\" else min\n        col = -1 if metric is \"accuracy\" else -2\n        data = np.genfromtxt(csv_path, delimiter=\",\")\n        row = list(data[:, col]).index(best(data[:, col]))\n        genome = list(map(int, data[row, :-2]))\n        if include_metrics:\n            genome += list(data[row, -2:])\n        return genome\n\n    def decode_best(self, csv_path, metric=\"accuracy\"):\n        return self.decode(self.best_genome(csv_path, metric, False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6fd6df4-558f-45f4-abc6-e230a4c199cd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# DEvol On MNIST Results\n### Genome Record"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"984577c5-32d9-4b8f-b144-cfd5dfc1c80e"}}},{"cell_type":"code","source":["(x_train, y_train), (x_test, y_test) = mnist.load_data()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e25f5feb-08b5-4721-96db-6dfc157a9eaf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["K.set_image_data_format(\"channels_last\")\n\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\ndataset = ((x_train, y_train), (x_test, y_test))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fb74443-e2fd-4e6d-ad6b-2b502e8011f2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["genome_handler = GenomeHandler(max_conv_layers=6, \n                               max_dense_layers=2, # includes final dense layer\n                               max_filters=256,\n                               max_dense_nodes=1024,\n                               input_shape=x_train.shape[1:],\n                               n_classes=10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c67b767b-782f-4e6c-abe9-d427b7f01376"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["devol = DEvol(genome_handler)\ndevol.run(dataset=dataset,\n                  num_generations=10,\n                  pop_size=4,\n                  epochs=1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3d5697a6-58c6-4a4b-92b8-ebf1a25e095f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Genome encoding and metric data stored at /dbfs/FileStore/runs/Sat_Nov_28_00-58-29_2020 \n\n1, 16, 1, 0, 10, 1, 0, 128, 1, 0, 6, 2, 0, 256, 0, 1, 2, 2, 1, 32, 1, 1, 6, 1, 0, 64, 1, 0, 3, 0, 0, 64, 1, 1, 3, 1, 0, 32, 1, 0, 5, 0\n1, 32, 1, 1, 1, 2, 1, 256, 1, 1, 4, 2, 0, 256, 1, 0, 6, 0, 0, 128, 1, 1, 4, 1, 0, 256, 0, 1, 4, 0, 0, 32, 1, 1, 6, 2, 1, 1024, 1, 1, 5, 3\n1, 64, 0, 1, 0, 1, 0, 32, 0, 0, 4, 1, 1, 32, 1, 1, 4, 1, 0, 64, 0, 1, 0, 0, 0, 32, 0, 0, 4, 2, 1, 64, 1, 0, 7, 1, 0, 32, 0, 1, 8, 0\n1, 128, 0, 0, 7, 0, 1, 64, 0, 0, 2, 0, 0, 8, 1, 0, 1, 0, 0, 32, 1, 1, 0, 0, 0, 16, 0, 1, 7, 0, 0, 128, 1, 0, 8, 2, 1, 256, 0, 0, 3, 3\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Genome encoding and metric data stored at /dbfs/FileStore/runs/Sat_Nov_28_00-58-29_2020 \n\n1, 16, 1, 0, 10, 1, 0, 128, 1, 0, 6, 2, 0, 256, 0, 1, 2, 2, 1, 32, 1, 1, 6, 1, 0, 64, 1, 0, 3, 0, 0, 64, 1, 1, 3, 1, 0, 32, 1, 0, 5, 0\n1, 32, 1, 1, 1, 2, 1, 256, 1, 1, 4, 2, 0, 256, 1, 0, 6, 0, 0, 128, 1, 1, 4, 1, 0, 256, 0, 1, 4, 0, 0, 32, 1, 1, 6, 2, 1, 1024, 1, 1, 5, 3\n1, 64, 0, 1, 0, 1, 0, 32, 0, 0, 4, 1, 1, 32, 1, 1, 4, 1, 0, 64, 0, 1, 0, 0, 0, 32, 0, 0, 4, 2, 1, 64, 1, 0, 7, 1, 0, 32, 0, 1, 8, 0\n1, 128, 0, 0, 7, 0, 1, 64, 0, 0, 2, 0, 0, 8, 1, 0, 1, 0, 0, 32, 1, 1, 0, 0, 0, 16, 0, 1, 7, 0, 0, 128, 1, 0, 8, 2, 1, 256, 0, 0, 3, 3\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4275497423460332&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>                   num_generations<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>                   pop_size<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">4</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">                   epochs=1)\n</span>\n<span class=\"ansi-green-fg\">&lt;command-4275497423459875&gt;</span> in <span class=\"ansi-cyan-fg\">run</span><span class=\"ansi-blue-fg\">(self, dataset, num_generations, pop_size, epochs, fitness, metric)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    127</span>                                         fitness<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>                                         <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">--&gt; 129</span><span class=\"ansi-red-fg\">                                         num_generations)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    130</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>         <span class=\"ansi-red-fg\"># evolve</span>\n\n<span class=\"ansi-green-fg\">&lt;command-4275497423459875&gt;</span> in <span class=\"ansi-cyan-fg\">_evaluate_population</span><span class=\"ansi-blue-fg\">(self, members, epochs, fitness, igen, ngen)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    240</span>         df1 <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>df1<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">[</span> <span class=\"ansi-blue-fg\">&#34;v&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    241</span> \n<span class=\"ansi-green-fg\">--&gt; 242</span><span class=\"ansi-red-fg\">         </span>result <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>p_eval<span class=\"ansi-blue-fg\">(</span>df1<span class=\"ansi-blue-fg\">,</span> epochs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    243</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    244</span> \n\n<span class=\"ansi-green-fg\">&lt;command-4275497423459875&gt;</span> in <span class=\"ansi-cyan-fg\">p_eval</span><span class=\"ansi-blue-fg\">(self, df1, epochs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    224</span>       lam <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">lambda</span> row<span class=\"ansi-blue-fg\">:</span> transform_row<span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    225</span> \n<span class=\"ansi-green-fg\">--&gt; 226</span><span class=\"ansi-red-fg\">       </span>loss_acc_rdd <span class=\"ansi-blue-fg\">=</span> df1<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>lam<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    227</span>       <span class=\"ansi-green-fg\">return</span> loss_acc_rdd\n<span class=\"ansi-green-intense-fg ansi-bold\">    228</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    829</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    830</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 831</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    832</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    833</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 62, 10.139.64.12, executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 513, in dump_stream\n    write_int(len(bytes), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 837, in write_int\n    stream.write(struct.pack(&#34;!i&#34;, value))\nstruct.error: &#39;i&#39; format requires -2147483648 &lt;= number &lt;= 2147483647\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$17.apply(Executor.scala:606)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:612)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2346)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:996)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:247)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor501.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 513, in dump_stream\n    write_int(len(bytes), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 837, in write_int\n    stream.write(struct.pack(&#34;!i&#34;, value))\nstruct.error: &#39;i&#39; format requires -2147483648 &lt;= number &lt;= 2147483647\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$17.apply(Executor.scala:606)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:612)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 62, 10.139.64.12, executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-4275497423460332&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      3</span>                   num_generations<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">10</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span>                   pop_size<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">4</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">----&gt; 5</span><span class=\"ansi-red-fg\">                   epochs=1)\n</span>\n<span class=\"ansi-green-fg\">&lt;command-4275497423459875&gt;</span> in <span class=\"ansi-cyan-fg\">run</span><span class=\"ansi-blue-fg\">(self, dataset, num_generations, pop_size, epochs, fitness, metric)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    127</span>                                         fitness<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>                                         <span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-fg\">--&gt; 129</span><span class=\"ansi-red-fg\">                                         num_generations)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    130</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    131</span>         <span class=\"ansi-red-fg\"># evolve</span>\n\n<span class=\"ansi-green-fg\">&lt;command-4275497423459875&gt;</span> in <span class=\"ansi-cyan-fg\">_evaluate_population</span><span class=\"ansi-blue-fg\">(self, members, epochs, fitness, igen, ngen)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    240</span>         df1 <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>createDataFrame<span class=\"ansi-blue-fg\">(</span>df1<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">[</span> <span class=\"ansi-blue-fg\">&#34;v&#34;</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    241</span> \n<span class=\"ansi-green-fg\">--&gt; 242</span><span class=\"ansi-red-fg\">         </span>result <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>p_eval<span class=\"ansi-blue-fg\">(</span>df1<span class=\"ansi-blue-fg\">,</span> epochs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    243</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    244</span> \n\n<span class=\"ansi-green-fg\">&lt;command-4275497423459875&gt;</span> in <span class=\"ansi-cyan-fg\">p_eval</span><span class=\"ansi-blue-fg\">(self, df1, epochs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    224</span>       lam <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">lambda</span> row<span class=\"ansi-blue-fg\">:</span> transform_row<span class=\"ansi-blue-fg\">(</span>row<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    225</span> \n<span class=\"ansi-green-fg\">--&gt; 226</span><span class=\"ansi-red-fg\">       </span>loss_acc_rdd <span class=\"ansi-blue-fg\">=</span> df1<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">.</span>map<span class=\"ansi-blue-fg\">(</span>lam<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>collect<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    227</span>       <span class=\"ansi-green-fg\">return</span> loss_acc_rdd\n<span class=\"ansi-green-intense-fg ansi-bold\">    228</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/rdd.py</span> in <span class=\"ansi-cyan-fg\">collect</span><span class=\"ansi-blue-fg\">(self)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    829</span>         <span class=\"ansi-red-fg\"># Default path used in OSS Spark / for non-credential passthrough clusters:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    830</span>         <span class=\"ansi-green-fg\">with</span> SCCallSiteSync<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>context<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">as</span> css<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 831</span><span class=\"ansi-red-fg\">             </span>sock_info <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>ctx<span class=\"ansi-blue-fg\">.</span>_jvm<span class=\"ansi-blue-fg\">.</span>PythonRDD<span class=\"ansi-blue-fg\">.</span>collectAndServe<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jrdd<span class=\"ansi-blue-fg\">.</span>rdd<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    832</span>         <span class=\"ansi-green-fg\">return</span> list<span class=\"ansi-blue-fg\">(</span>_load_from_socket<span class=\"ansi-blue-fg\">(</span>sock_info<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>_jrdd_deserializer<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    833</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1255</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1256</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1257</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1258</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1259</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     61</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     62</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 63</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     64</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     65</span>             s <span class=\"ansi-blue-fg\">=</span> e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">.</span>toString<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 62, 10.139.64.12, executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 513, in dump_stream\n    write_int(len(bytes), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 837, in write_int\n    stream.write(struct.pack(&#34;!i&#34;, value))\nstruct.error: &#39;i&#39; format requires -2147483648 &lt;= number &lt;= 2147483647\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$17.apply(Executor.scala:606)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:612)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2362)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2350)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2349)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2349)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1102)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1102)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2582)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2529)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2517)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2280)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2302)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2346)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:392)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:996)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:247)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor501.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 480, in main\n    process()\n  File &#34;/databricks/spark/python/pyspark/worker.py&#34;, line 472, in process\n    serializer.dump_stream(out_iter, outfile)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 513, in dump_stream\n    write_int(len(bytes), stream)\n  File &#34;/databricks/spark/python/pyspark/serializers.py&#34;, line 837, in write_int\n    stream.write(struct.pack(&#34;!i&#34;, value))\nstruct.error: &#39;i&#39; format requires -2147483648 &lt;= number &lt;= 2147483647\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:540)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:676)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:659)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:494)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$15.apply(RDD.scala:997)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2321)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:140)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:113)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$17.apply(Executor.scala:606)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:612)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"53f05301-33cb-4a01-a86b-c46a31db4162"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Genome encoding and metric data stored at /dbfs/FileStore/runs/Sat_Nov_28_01-44-49_2020 \n\n1, 8, 0, 1, 8, 1, 1, 8, 1, 0, 4, 2, 0, 8, 1, 1, 7, 2, 0, 16, 0, 1, 6, 0, 0, 32, 0, 1, 1, 1, 1, 64, 1, 0, 3, 0, 1, 64, 1, 0, 9, 2\n1, 128, 0, 1, 3, 2, 1, 32, 1, 1, 8, 1, 0, 8, 1, 0, 8, 2, 0, 256, 0, 1, 1, 1, 0, 64, 0, 0, 8, 2, 1, 32, 1, 0, 2, 0, 0, 16, 0, 0, 7, 2\n1, 256, 0, 1, 2, 1, 0, 32, 1, 1, 3, 1, 1, 256, 0, 1, 10, 1, 1, 256, 0, 1, 7, 1, 0, 64, 1, 1, 8, 2, 0, 64, 1, 0, 10, 1, 1, 1024, 1, 1, 7, 2\n1, 64, 1, 0, 0, 1, 0, 8, 0, 0, 3, 1, 1, 16, 0, 0, 3, 0, 0, 256, 0, 1, 6, 1, 0, 64, 0, 0, 2, 2, 0, 16, 1, 0, 6, 1, 0, 128, 1, 1, 9, 3\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Genome encoding and metric data stored at /dbfs/FileStore/runs/Sat_Nov_28_01-44-49_2020 \n\n1, 8, 0, 1, 8, 1, 1, 8, 1, 0, 4, 2, 0, 8, 1, 1, 7, 2, 0, 16, 0, 1, 6, 0, 0, 32, 0, 1, 1, 1, 1, 64, 1, 0, 3, 0, 1, 64, 1, 0, 9, 2\n1, 128, 0, 1, 3, 2, 1, 32, 1, 1, 8, 1, 0, 8, 1, 0, 8, 2, 0, 256, 0, 1, 1, 1, 0, 64, 0, 0, 8, 2, 1, 32, 1, 0, 2, 0, 0, 16, 0, 0, 7, 2\n1, 256, 0, 1, 2, 1, 0, 32, 1, 1, 3, 1, 1, 256, 0, 1, 10, 1, 1, 256, 0, 1, 7, 1, 0, 64, 1, 1, 8, 2, 0, 64, 1, 0, 10, 1, 1, 1024, 1, 1, 7, 2\n1, 64, 1, 0, 0, 1, 0, 8, 0, 0, 3, 1, 1, 16, 0, 0, 3, 0, 0, 256, 0, 1, 6, 1, 0, 64, 0, 0, 2, 2, 0, 16, 1, 0, 6, 1, 0, 128, 1, 1, 9, 3\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"42fb50d0-97eb-4044-bd44-20a2d5ed3445"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"devol_rdd_accelerated","dashboards":[],"language":"python","widgets":{},"notebookOrigID":4275497423459788}},"nbformat":4,"nbformat_minor":0}

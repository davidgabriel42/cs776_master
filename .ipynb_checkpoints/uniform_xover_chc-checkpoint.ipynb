{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bc46fb1f-06a7-4639-b0fd-3c735d5232b6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Using TensorFlow backend.\n",
       "/databricks/python/lib/python3.7/site-packages/botocore/vendored/requests/packages/urllib3/_collections.py:1: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working\n",
       "  from collections import Mapping, MutableMapping\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Using TensorFlow backend.\n/databricks/python/lib/python3.7/site-packages/botocore/vendored/requests/packages/urllib3/_collections.py:1: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working\n  from collections import Mapping, MutableMapping\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import random as rand\n",
    "import csv\n",
    "import operator\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "\n",
    "if K.backend() == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "from setuptools import setup, find_packages\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from devol import DEvol, GenomeHandler\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import *\n",
    "import inspect\n",
    "from typing import Callable, List\n",
    "import pandas as pd\n",
    "from pyspark.sql import DataFrame, Row, column\n",
    "from pyspark.sql.functions import lit, pandas_udf, PandasUDFType, array\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d016ffbb-9cc3-4c58-bfac-0e9b1b5729fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle \n",
    "pickle_file = open('/dbfs/FileStore/models/simple/pickle','rb')\n",
    "retrieved_devol = pickle.load(pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba88812a-6094-4959-85ec-73a7c98aac91",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#use rdd to parallelize training\n",
    "def transform_row(row):\n",
    "  model = None\n",
    "  loss = None\n",
    "  accurcacy = None\n",
    "  accuracy = 0\n",
    "  gene = row.v\n",
    "  gene = gene.split(sep = ',')\n",
    "  result = []\n",
    "  for item in gene:\n",
    "    result.append(int(item))\n",
    "    \n",
    "  if(genome_handler.is_compatible_genome(result)):\n",
    "    model, loss, accuracy = retrieved_devol._evaluate(result,1)\n",
    "  return model, loss, accuracy\n",
    "lam = lambda row: transform_row(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "75a4096c-033c-470a-bc0c-36f2752cc845",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">GPU accelerated\n",
       "Num GPUs Available:  2\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">GPU accelerated\nNum GPUs Available:  2\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if(len(tf.config.experimental.list_physical_devices('GPU')) > 0):\n",
    "  print(\"GPU accelerated\")\n",
    "  print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "else:\n",
    "  print(\"CPU mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9644b11b-46aa-434d-80a2-a14addc2a2ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Run a genetic algorithm to find an appropriate architecture for some image\n",
    "classification task with Keras+TF.\n",
    "To use, define a `GenomeHandler` defined in genomehandler.py. Then pass it, with\n",
    "training data, to a DEvol instance to run the genetic algorithm. See the readme\n",
    "for more detailed instructions.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import random as rand\n",
    "import csv\n",
    "import operator\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "import keras.backend as K\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "if K.backend() == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "\n",
    "__all__ = ['DEvol']\n",
    "\n",
    "METRIC_OPS = [operator.__lt__, operator.__gt__]\n",
    "METRIC_OBJECTIVES = [min, max]\n",
    "\n",
    "#dbutils.fs.mkdirs( '/dbfs/FileStore/models/gpu/')\n",
    "\n",
    "\n",
    "class DEvol:\n",
    "    \"\"\"\n",
    "    Object which carries out genetic search and returns top performing model\n",
    "    upon completion.\n",
    "    \"\"\"\n",
    "#!!! data_path with checkpointing\n",
    "    def __init__(self, genome_handler, data_path=\"/dbfs/FileStore/runs/uniform_xover_25chc\"):\n",
    "        \"\"\"\n",
    "        Initialize a DEvol object which carries out the training and evaluation\n",
    "        of a genetic search.\n",
    "        Args:\n",
    "            genome_handler (GenomeHandler): the genome handler object defining\n",
    "                    the restrictions for the architecture search space\n",
    "            data_path (str): the file which the genome encodings and metric data\n",
    "                    will be stored in\n",
    "        \"\"\"\n",
    "        self.data_path = data_path + datetime.now().ctime().replace(\":\",\"-\").replace(\" \",\"_\")\n",
    "        data_path = self.data_path\n",
    "        dbutils.fs.mkdirs(data_path )\n",
    "        \n",
    "        self.genome_handler = genome_handler\n",
    "        self.datafile = data_path or (data_path + 'record.csv')\n",
    "        self._bssf = -1\n",
    "\n",
    "\n",
    "\n",
    "        if os.path.isfile(data_path) and os.stat(data_path).st_size > 1:\n",
    "            raise ValueError(('Non-empty file %s already exists. Please change'\n",
    "                              'file path to prevent overwritten genome data.'\n",
    "                              % data_path))\n",
    "\n",
    "        print(\"Genome encoding and metric data stored at\", self.datafile, \"\\n\")\n",
    "        with open(self.datafile, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',', quotechar='\"',\n",
    "                                quoting=csv.QUOTE_MINIMAL)\n",
    "            metric_cols = [\"Val Loss\", \"Val Accuracy\"]\n",
    "            genome = genome_handler.genome_representation() + metric_cols\n",
    "            writer.writerow(genome)\n",
    "\n",
    "    def set_objective(self, metric):\n",
    "        \"\"\"\n",
    "        Set the metric for optimization. Can also be done by passing to\n",
    "        `run`.\n",
    "        Args:\n",
    "            metric (str): either 'acc' to maximize classification accuracy, or\n",
    "                    else 'loss' to minimize the loss function\n",
    "        \"\"\"\n",
    "        if metric == 'acc':\n",
    "            metric = 'accuracy'\n",
    "        if metric not in ['loss', 'accuracy']:\n",
    "            raise ValueError(('Invalid metric name {} provided - should be'\n",
    "                              '\"accuracy\" or \"loss\"').format(metric))\n",
    "        self._metric = metric\n",
    "        self._objective = \"max\" if self._metric == \"accuracy\" else \"min\"\n",
    "        self._metric_index = 1 if self._metric == 'loss' else -1\n",
    "        self._metric_op = METRIC_OPS[self._objective == 'max']\n",
    "        self._metric_objective = METRIC_OBJECTIVES[self._objective == 'max']\n",
    "       \n",
    "        \n",
    "    def run(self, dataset, num_generations, pop_size, epochs, fitness=None,\n",
    "            metric='accuracy'):\n",
    "        \"\"\"\n",
    "        Run genetic search on dataset given number of generations and\n",
    "        population size\n",
    "        Args:\n",
    "            dataset : tuple or list of numpy arrays in form ((train_data,\n",
    "                    train_labels), (validation_data, validation_labels))\n",
    "            num_generations (int): number of generations to search\n",
    "            pop_size (int): initial population size\n",
    "            epochs (int): epochs for each model eval, passed to keras model.fit\n",
    "            fitness (None, optional): scoring function to be applied to\n",
    "                    population scores, will be called on a numpy array which is\n",
    "                    a min/max scaled version of evaluated model metrics, so It\n",
    "                    should accept a real number including 0. If left as default\n",
    "                    just the min/max scaled values will be used.\n",
    "            metric (str, optional): must be \"accuracy\" or \"loss\" , defines what\n",
    "                    to optimize during search\n",
    "        Returns:\n",
    "            keras model: best model found with weights\n",
    "        \"\"\"\n",
    "        self.set_objective(metric)\n",
    "\n",
    "        # If no validation data is given set it to None\n",
    "        if len(dataset) == 2:\n",
    "            (self.x_train, self.y_train), (self.x_test, self.y_test) = dataset\n",
    "            self.x_val = None\n",
    "            self.y_val = None\n",
    "        else:\n",
    "            (self.x_train, self.y_train), (self.x_test, self.y_test), (self.x_val, self.y_val) = dataset\n",
    "\n",
    "        # generate and evaluate initial population\n",
    "        members = self._generate_random_population(pop_size)\n",
    "        pop = self._evaluate_population(members,\n",
    "                                        epochs,\n",
    "                                        fitness,\n",
    "                                        0,\n",
    "                                        num_generations)\n",
    "\n",
    "        # evolve\n",
    "        for gen in range(1, num_generations):\n",
    "            members = self._reproduce(pop, gen)\n",
    "            #!!! map to pandas df, apply udf parallel training, save scores\n",
    "            pop = self._evaluate_population(members,\n",
    "                                            epochs,\n",
    "                                            fitness,\n",
    "                                            gen,\n",
    "                                            num_generations)\n",
    "        \n",
    "        #!!!add checkpointing to dbfs\n",
    "\n",
    "        return 1\n",
    "        #return load_model('best-model.h5')\n",
    "\n",
    "    def _reproduce(self, pop, gen):\n",
    "        members = []\n",
    "\n",
    "        # 2 children from crossover\n",
    "        for _ in range(int(len(pop) * 0.40)):\n",
    "            members.append(self._uniform_crossover(pop.select(), pop.select()))\n",
    "\n",
    "        # best models survive automatically 20%\n",
    "        members += pop.get_best(len(pop) - int(len(pop) * 0.8))\n",
    "\n",
    "        # randomly mutate\n",
    "        for imem, mem in enumerate(members):\n",
    "            members[imem] = self._mutate(mem, gen)\n",
    "        return members\n",
    "\n",
    "    def _evaluate(self, genome, epochs):\n",
    "        model = self.genome_handler.decode(genome)\n",
    "        loss, accuracy = None, None\n",
    "        fit_params = {\n",
    "            'x': self.x_train,\n",
    "            'y': self.y_train,\n",
    "            'validation_split': 0.1,\n",
    "            'epochs': epochs,\n",
    "            'verbose': 1,\n",
    "            'callbacks': [\n",
    "                EarlyStopping(monitor='val_loss', patience=1, verbose=1)\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        if self.x_val is not None:\n",
    "            fit_params['validation_data'] = (self.x_val, self.y_val)\n",
    "        try:\n",
    "            model.fit(**fit_params)\n",
    "            loss, accuracy = model.evaluate(self.x_test, self.y_test, verbose=0)\n",
    "        except Exception as e:\n",
    "            loss, accuracy = self._handle_broken_model(model, e)\n",
    "\n",
    "        self._record_stats(model, genome, loss, accuracy)\n",
    "\n",
    "        return model, loss, accuracy\n",
    "\n",
    "    def _record_stats(self, model, genome, loss, accuracy):\n",
    "        with open(self.datafile, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',',\n",
    "                                quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            row = list(genome) + [loss, accuracy]\n",
    "            writer.writerow(row)\n",
    "\n",
    "        met = loss if self._metric == 'loss' else accuracy\n",
    "        if (self._bssf is -1 or\n",
    "                self._metric_op(met, self._bssf) and\n",
    "                accuracy is not 0):\n",
    "            \n",
    "            self._bssf = met\n",
    "            #azure enforces mlflow saving\n",
    "            mlflow.keras.save_model(model, self.data_path + '/best-model-mlflow')\n",
    "\n",
    "    def _handle_broken_model(self, model, error):\n",
    "        del model\n",
    "\n",
    "        n = self.genome_handler.n_classes\n",
    "        loss = log_loss(np.concatenate(([1], np.zeros(n - 1))), np.ones(n) / n)\n",
    "        accuracy = 1 / n\n",
    "        gc.collect()\n",
    "\n",
    "        if K.backend() == 'tensorflow':\n",
    "            K.clear_session()\n",
    "            tf.reset_default_graph()\n",
    "\n",
    "        print('An error occurred and the model could not train:')\n",
    "        print(error)\n",
    "        print(('Model assigned poor score. Please ensure that your model'\n",
    "               'constraints live within your computational resources.'))\n",
    "        return loss, accuracy\n",
    "\n",
    "    def  p_eval(self, df1, epochs):\n",
    "      #use rdd to parallelize training\n",
    "      \n",
    "      lam = lambda row: transform_row(row)\n",
    "\n",
    "      loss_acc_rdd = df1.rdd.map(lam).collect()\n",
    "      return loss_acc_rdd\n",
    "    \n",
    "    def _evaluate_population(self, members, epochs, fitness, igen, ngen):\n",
    "        df1 = pd.DataFrame()\n",
    "        list= []\n",
    "        for member in members:\n",
    "            gene = (str(member))[1:-1] \n",
    "            sep = ''\n",
    "            foo=(sep.join(gene))\n",
    "            list.append(foo)\n",
    "            print(foo)\n",
    "\n",
    "        df1 = pd.DataFrame(list)\n",
    "        df1 = spark.createDataFrame(df1, [ \"v\"])\n",
    "        \n",
    "        result = self.p_eval(df1, epochs)\n",
    "        \n",
    "        \n",
    "        fit = []\n",
    "        #data_path = \"/dbfs/FileStore/test/\"\n",
    "        #result2\n",
    "        for imem, mem in enumerate(members):\n",
    "          model = result[imem][0]\n",
    "          loss = result[imem][1]\n",
    "          accuracy = result[imem][2]\n",
    "          name = datetime.now().ctime() + \"_\" + str(accuracy) + \"_\" +str(loss)\n",
    "          \n",
    "          mlflow.keras.save_model(model,(self.data_path+ name))\n",
    "          #model.save_model(data_path + name)\n",
    "          print(\"saved :\",name,\"\\n\")\n",
    "          res = model, loss, accuracy\n",
    "          v = res[self._metric_index]\n",
    "          print(res[self._metric_index])\n",
    "          del res\n",
    "          fit.append(v)\n",
    "        \n",
    "        fit = np.array(fit)\n",
    "        self._print_result(fit, igen)\n",
    "        return _Population(members, fit, fitness, obj=self._objective)\n",
    "\n",
    "    def _print_evaluation(self, imod, nmod, igen, ngen):\n",
    "        fstr = '\\nmodel {0}/{1} - generation {2}/{3}:\\n'\n",
    "        print(fstr.format(imod + 1, nmod, igen + 1, ngen))\n",
    "\n",
    "    def _generate_random_population(self, size):\n",
    "        return [self.genome_handler.generate() for _ in range(size)]\n",
    "\n",
    "    def _print_result(self, fitness, generation):\n",
    "        result_str = ('Generation {3}:\\t\\tbest {4}: {0:0.4f}\\t\\taverage:'\n",
    "                      '{1:0.4f}\\t\\tstd: {2:0.4f}')\n",
    "        print(result_str.format(self._metric_objective(fitness),\n",
    "                                np.mean(fitness),\n",
    "                                np.std(fitness),\n",
    "                                generation + 1, self._metric))\n",
    "\n",
    "    def _crossover(self, genome1, genome2):\n",
    "        cross_ind = rand.randint(0, len(genome1))\n",
    "        child = genome1[:cross_ind] + genome2[cross_ind:]\n",
    "        return child\n",
    "\n",
    "    def _uniform_crossover(self,genome1,genome2):\n",
    "        prob_xover = 0.5\n",
    "        for i in range(len(genome1)):\n",
    "          if rand.random() < prob_xover:\n",
    "            genome1[i], genome2[i] = genome2[i], genome1[i]\n",
    "        return genome1, genome2\n",
    "      \n",
    "    def _mutate(self, genome, generation):\n",
    "        # increase mutations as program continues\n",
    "        num_mutations = max(3, generation // 4)\n",
    "        return self.genome_handler.mutate(genome, num_mutations)\n",
    "\n",
    "\n",
    "class _Population(object):\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.members)\n",
    "\n",
    "    def __init__(self, members, fitnesses, score, obj='max'):\n",
    "        self.members = members\n",
    "        scores = fitnesses - fitnesses.min()\n",
    "        if scores.max() > 0:\n",
    "            scores /= scores.max()\n",
    "        if obj == 'min':\n",
    "            scores = 1 - scores\n",
    "        if score:\n",
    "            self.scores = score(scores)\n",
    "        else:\n",
    "            self.scores = scores\n",
    "        self.s_fit = sum(self.scores)\n",
    "\n",
    "    def get_best(self, n):\n",
    "        combined = [(self.members[i], self.scores[i])\n",
    "                    for i in range(len(self.members))]\n",
    "        sorted(combined, key=(lambda x: x[1]), reverse=True)\n",
    "        return [x[0] for x in combined[:n]]\n",
    "\n",
    "    \n",
    "      \n",
    "    #fitness proportional selection\n",
    "    def select(self):\n",
    "        dart = rand.uniform(0, self.s_fit)\n",
    "        sum_fits = 0\n",
    "        for i in range(len(self.members)):\n",
    "            sum_fits += self.scores[i]\n",
    "            if sum_fits >= dart:\n",
    "                return self.members[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6fd6df4-558f-45f4-abc6-e230a4c199cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "class GenomeHandler:\n",
    "    \"\"\"\n",
    "    Defines the configuration and handles the conversion and mutation of\n",
    "    individual genomes. Should be created and passed to a `DEvol` instance.\n",
    "    ---\n",
    "    Genomes are represented as fixed-with lists of integers corresponding\n",
    "    to sequential layers and properties. A model with 2 convolutional layers\n",
    "    and 1 dense layer would look like:\n",
    "    [<conv layer><conv layer><dense layer><optimizer>]\n",
    "    The makeup of the convolutional layers and dense layers is defined in the\n",
    "    GenomeHandler below under self.convolutional_layer_shape and\n",
    "    self.dense_layer_shape. <optimizer> consists of just one property.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_conv_layers, max_dense_layers, max_filters,\n",
    "                 max_dense_nodes, input_shape, n_classes,\n",
    "                 batch_normalization=True, dropout=True, max_pooling=True,\n",
    "                 optimizers=None, activations=None):\n",
    "        \"\"\"\n",
    "        Creates a GenomeHandler according\n",
    "        Args:\n",
    "            max_conv_layers: The maximum number of convolutional layers\n",
    "            max_dense_layers: The maximum number of dense (fully connected)\n",
    "                    layers, including output layer\n",
    "            max_filters: The maximum number of conv filters (feature maps) in a\n",
    "                    convolutional layer\n",
    "            max_dense_nodes: The maximum number of nodes in a dense layer\n",
    "            input_shape: The shape of the input\n",
    "            n_classes: The number of classes\n",
    "            batch_normalization (bool): whether the GP should include batch norm\n",
    "            dropout (bool): whether the GP should include dropout\n",
    "            max_pooling (bool): whether the GP should include max pooling layers\n",
    "            optimizers (list): list of optimizers to be tried by the GP. By\n",
    "                    default, the network uses Keras's built-in adam, rmsprop,\n",
    "                    adagrad, and adadelta\n",
    "            activations (list): list of activation functions to be tried by the\n",
    "                    GP. By default, relu and sigmoid.\n",
    "        \"\"\"\n",
    "        if max_dense_layers < 1:\n",
    "            raise ValueError(\n",
    "                \"At least one dense layer is required for softmax layer\"\n",
    "            )\n",
    "        if max_filters > 0:\n",
    "            filter_range_max = int(math.log(max_filters, 2)) + 1\n",
    "        else:\n",
    "            filter_range_max = 0\n",
    "        self.optimizer = optimizers or [\n",
    "            'adam',\n",
    "            'rmsprop',\n",
    "            'adagrad',\n",
    "            'adadelta'\n",
    "        ]\n",
    "        self.activation = activations or [\n",
    "            'relu',\n",
    "            'sigmoid',\n",
    "        ]\n",
    "        self.convolutional_layer_shape = [\n",
    "            \"active\",\n",
    "            \"num filters\",\n",
    "            \"batch normalization\",\n",
    "            \"activation\",\n",
    "            \"dropout\",\n",
    "            \"max pooling\",\n",
    "        ]\n",
    "        self.dense_layer_shape = [\n",
    "            \"active\",\n",
    "            \"num nodes\",\n",
    "            \"batch normalization\",\n",
    "            \"activation\",\n",
    "            \"dropout\",\n",
    "        ]\n",
    "        self.layer_params = {\n",
    "            \"active\": [0, 1],\n",
    "            \"num filters\": [2**i for i in range(3, filter_range_max)],\n",
    "            \"num nodes\": [2**i for i in range(4, int(math.log(max_dense_nodes, 2)) + 1)],\n",
    "            \"batch normalization\": [0, (1 if batch_normalization else 0)],\n",
    "            \"activation\": list(range(len(self.activation))),\n",
    "            \"dropout\": [(i if dropout else 0) for i in range(11)],\n",
    "            \"max pooling\": list(range(3)) if max_pooling else 0,\n",
    "        }\n",
    "\n",
    "        self.convolution_layers = max_conv_layers\n",
    "        self.convolution_layer_size = len(self.convolutional_layer_shape)\n",
    "        self.dense_layers = max_dense_layers - 1  # this doesn't include the softmax layer, so -1\n",
    "        self.dense_layer_size = len(self.dense_layer_shape)\n",
    "        self.input_shape = input_shape\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def convParam(self, i):\n",
    "        key = self.convolutional_layer_shape[i]\n",
    "        return self.layer_params[key]\n",
    "\n",
    "    def denseParam(self, i):\n",
    "        key = self.dense_layer_shape[i]\n",
    "        return self.layer_params[key]\n",
    "\n",
    "    def mutate(self, genome, num_mutations):\n",
    "        num_mutations = np.random.choice(num_mutations)\n",
    "        for i in range(num_mutations):\n",
    "            index = np.random.choice(list(range(1, len(genome))))\n",
    "            if index < self.convolution_layer_size * self.convolution_layers:\n",
    "                if genome[index - index % self.convolution_layer_size]:\n",
    "                    range_index = index % self.convolution_layer_size\n",
    "                    choice_range = self.convParam(range_index)\n",
    "                    genome[index] = np.random.choice(choice_range)\n",
    "                elif rand.uniform(0, 1) <= 0.01:  # randomly flip deactivated layers\n",
    "                    genome[index - index % self.convolution_layer_size] = 1\n",
    "            elif index != len(genome) - 1:\n",
    "                offset = self.convolution_layer_size * self.convolution_layers\n",
    "                new_index = (index - offset)\n",
    "                present_index = new_index - new_index % self.dense_layer_size\n",
    "                if genome[present_index + offset]:\n",
    "                    range_index = new_index % self.dense_layer_size\n",
    "                    choice_range = self.denseParam(range_index)\n",
    "                    genome[index] = np.random.choice(choice_range)\n",
    "                elif rand.uniform(0, 1) <= 0.01:\n",
    "                    genome[present_index + offset] = 1\n",
    "            else:\n",
    "                genome[index] = np.random.choice(list(range(len(self.optimizer))))\n",
    "        return genome\n",
    "\n",
    "    def decode(self, genome):\n",
    "        if not self.is_compatible_genome(genome):\n",
    "            raise ValueError(\"Invalid genome for specified configs\")\n",
    "        model = Sequential()\n",
    "        dim = 0\n",
    "        offset = 0\n",
    "        if self.convolution_layers > 0:\n",
    "            dim = min(self.input_shape[:-1])  # keep track of smallest dimension\n",
    "        input_layer = True\n",
    "        for i in range(self.convolution_layers):\n",
    "            if genome[offset]:\n",
    "                convolution = None\n",
    "                if input_layer:\n",
    "                    convolution = Convolution2D(\n",
    "                        genome[offset + 1], (3, 3),\n",
    "                        padding='same',\n",
    "                        input_shape=self.input_shape\n",
    "                    )\n",
    "                    input_layer = False\n",
    "                else:\n",
    "                    convolution = Convolution2D(\n",
    "                        genome[offset + 1], (3, 3),\n",
    "                        padding='same'\n",
    "                    )\n",
    "                model.add(convolution)\n",
    "                if genome[offset + 2]:\n",
    "                    model.add(BatchNormalization())\n",
    "                model.add(Activation(self.activation[genome[offset + 3]]))\n",
    "                model.add(Dropout(float(genome[offset + 4] / 20.0)))\n",
    "                max_pooling_type = genome[offset + 5]\n",
    "                # must be large enough for a convolution\n",
    "                if max_pooling_type == 1 and dim >= 5:\n",
    "                    model.add(MaxPooling2D(pool_size=(2, 2), padding=\"same\"))\n",
    "                    dim = int(math.ceil(dim / 2))\n",
    "            offset += self.convolution_layer_size\n",
    "\n",
    "        if not input_layer:\n",
    "            model.add(Flatten())\n",
    "\n",
    "        for i in range(self.dense_layers):\n",
    "            if genome[offset]:\n",
    "                dense = None\n",
    "                if input_layer:\n",
    "                    dense = Dense(genome[offset + 1], input_shape=self.input_shape)\n",
    "                    input_layer = False\n",
    "                else:\n",
    "                    dense = Dense(genome[offset + 1])\n",
    "                model.add(dense)\n",
    "                if genome[offset + 2]:\n",
    "                    model.add(BatchNormalization())\n",
    "                model.add(Activation(self.activation[genome[offset + 3]]))\n",
    "                model.add(Dropout(float(genome[offset + 4] / 20.0)))\n",
    "            offset += self.dense_layer_size\n",
    "\n",
    "        model.add(Dense(self.n_classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=self.optimizer[genome[offset]],\n",
    "                      metrics=[\"accuracy\"])\n",
    "        return model\n",
    "\n",
    "    def genome_representation(self):\n",
    "        encoding = []\n",
    "        for i in range(self.convolution_layers):\n",
    "            for key in self.convolutional_layer_shape:\n",
    "                encoding.append(\"Conv\" + str(i) + \" \" + key)\n",
    "        for i in range(self.dense_layers):\n",
    "            for key in self.dense_layer_shape:\n",
    "                encoding.append(\"Dense\" + str(i) + \" \" + key)\n",
    "        encoding.append(\"Optimizer\")\n",
    "        return encoding\n",
    "\n",
    "    def generate(self):\n",
    "        genome = []\n",
    "        for i in range(self.convolution_layers):\n",
    "            for key in self.convolutional_layer_shape:\n",
    "                param = self.layer_params[key]\n",
    "                genome.append(np.random.choice(param))\n",
    "        for i in range(self.dense_layers):\n",
    "            for key in self.dense_layer_shape:\n",
    "                param = self.layer_params[key]\n",
    "                genome.append(np.random.choice(param))\n",
    "        genome.append(np.random.choice(list(range(len(self.optimizer)))))\n",
    "        genome[0] = 1\n",
    "        return genome\n",
    "\n",
    "    def is_compatible_genome(self, genome):\n",
    "        expected_len = self.convolution_layers * self.convolution_layer_size \\\n",
    "            + self.dense_layers * self.dense_layer_size + 1\n",
    "        if len(genome) != expected_len:\n",
    "            return False\n",
    "        ind = 0\n",
    "        for i in range(self.convolution_layers):\n",
    "            for j in range(self.convolution_layer_size):\n",
    "                if genome[ind + j] not in self.convParam(j):\n",
    "                    return False\n",
    "            ind += self.convolution_layer_size\n",
    "        for i in range(self.dense_layers):\n",
    "            for j in range(self.dense_layer_size):\n",
    "                if genome[ind + j] not in self.denseParam(j):\n",
    "                    return False\n",
    "            ind += self.dense_layer_size\n",
    "        if genome[ind] not in range(len(self.optimizer)):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def best_genome(self, csv_path, metric=\"accuracy\", include_metrics=True):\n",
    "        best = max if metric is \"accuracy\" else min\n",
    "        col = -1 if metric is \"accuracy\" else -2\n",
    "        data = np.genfromtxt(csv_path, delimiter=\",\")\n",
    "        row = list(data[:, col]).index(best(data[:, col]))\n",
    "        genome = list(map(int, data[row, :-2]))\n",
    "        if include_metrics:\n",
    "            genome += list(data[row, -2:])\n",
    "        return genome\n",
    "\n",
    "    def decode_best(self, csv_path, metric=\"accuracy\"):\n",
    "        return self.decode(self.best_genome(csv_path, metric, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "984577c5-32d9-4b8f-b144-cfd5dfc1c80e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# DEvol On MNIST Results\n",
    "### Genome Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e25f5feb-08b5-4721-96db-6dfc157a9eaf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
       "\r",
       "    8192/11490434 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
       " 3129344/11490434 [=======&gt;......................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
       " 8691712/11490434 [=====================&gt;........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
       "11493376/11490434 [==============================] - 0s 0us/step\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n\r    8192/11490434 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 3129344/11490434 [=======&gt;......................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r 8691712/11490434 [=====================&gt;........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r11493376/11490434 [==============================] - 0s 0us/step\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6fb74443-e2fd-4e6d-ad6b-2b502e8011f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "K.set_image_data_format(\"channels_last\")\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "dataset = ((x_train, y_train), (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c67b767b-782f-4e6c-abe9-d427b7f01376",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "genome_handler = GenomeHandler(max_conv_layers=6, \n",
    "                               max_dense_layers=2, # includes final dense layer\n",
    "                               max_filters=256,\n",
    "                               max_dense_nodes=1024,\n",
    "                               input_shape=x_train.shape[1:],\n",
    "                               n_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b219cc3-09d1-4a39-9dc2-aebc5b9cd913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Genome encoding and metric data stored at /dbfs/FileStore/runs/uniform_xover_25chcFri_Nov_27_20-50-36_2020 \n",
       "\n",
       "1, 16, 0, 0, 8, 1, 1, 8, 0, 1, 10, 1, 1, 16, 0, 1, 9, 0, 1, 32, 1, 0, 3, 2, 0, 8, 1, 1, 7, 2, 1, 32, 1, 1, 9, 2, 1, 512, 1, 0, 10, 2\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
       "Instructions for updating:\n",
       "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
       "Instructions for updating:\n",
       "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
       "\n",
       "WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
       "\n",
       "saved : Fri Nov 27 20:51:25 2020_0.884_0.42739400658607485 \n",
       "\n",
       "0.884\n",
       "Generation 1:\t\tbest accuracy: 0.8840\t\taverage:0.8840\t\tstd: 0.0000\n",
       "1, 16, 0, 0, 8, 1, 1, 8, 0, 1, 10, 1, 1, 16, 0, 1, 9, 0, 1, 32, 1, 0, 3, 2, 0, 8, 1, 1, 7, 2, 1, 32, 1, 1, 9, 2, 1, 512, 1, 0, 10, 2\n",
       "saved : Fri Nov 27 20:51:57 2020_0.8846_0.42748521571159365 \n",
       "\n",
       "0.8846\n",
       "Generation 2:\t\tbest accuracy: 0.8846\t\taverage:0.8846\t\tstd: 0.0000\n",
       "Out[10]: 1</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Genome encoding and metric data stored at /dbfs/FileStore/runs/uniform_xover_25chcFri_Nov_27_20-50-36_2020 \n\n1, 16, 0, 0, 8, 1, 1, 8, 0, 1, 10, 1, 1, 16, 0, 1, 9, 0, 1, 32, 1, 0, 3, 2, 0, 8, 1, 1, 7, 2, 1, 32, 1, 1, 9, 2, 1, 512, 1, 0, 10, 2\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nWARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n\nsaved : Fri Nov 27 20:51:25 2020_0.884_0.42739400658607485 \n\n0.884\nGeneration 1:\t\tbest accuracy: 0.8840\t\taverage:0.8840\t\tstd: 0.0000\n1, 16, 0, 0, 8, 1, 1, 8, 0, 1, 10, 1, 1, 16, 0, 1, 9, 0, 1, 32, 1, 0, 3, 2, 0, 8, 1, 1, 7, 2, 1, 32, 1, 1, 9, 2, 1, 512, 1, 0, 10, 2\nsaved : Fri Nov 27 20:51:57 2020_0.8846_0.42748521571159365 \n\n0.8846\nGeneration 2:\t\tbest accuracy: 0.8846\t\taverage:0.8846\t\tstd: 0.0000\nOut[10]: 1</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=2,\n",
    "                  pop_size=1,\n",
    "                  epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3657abd5-b2c8-4fce-9684-5452bd3ba671",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Genome encoding and metric data stored at /dbfs/FileStore/runs/uniform_xover_25chcFri_Nov_27_20-51-59_2020 \n",
       "\n",
       "1, 64, 0, 0, 6, 1, 1, 256, 1, 1, 5, 0, 1, 128, 0, 1, 8, 0, 0, 64, 0, 0, 1, 1, 1, 32, 1, 0, 2, 1, 0, 128, 1, 1, 10, 2, 0, 16, 0, 1, 9, 3\n",
       "1, 8, 0, 0, 10, 0, 1, 256, 0, 1, 4, 1, 1, 8, 0, 1, 2, 1, 0, 256, 1, 0, 4, 1, 1, 8, 1, 1, 6, 1, 1, 32, 0, 0, 4, 0, 0, 64, 0, 1, 10, 2\n",
       "1, 128, 0, 1, 3, 1, 0, 32, 1, 0, 6, 1, 1, 256, 0, 1, 2, 1, 1, 64, 1, 0, 4, 0, 0, 128, 1, 1, 10, 0, 0, 16, 0, 0, 1, 2, 0, 128, 0, 1, 4, 1\n",
       "1, 16, 0, 0, 3, 0, 0, 16, 1, 0, 5, 1, 1, 128, 0, 1, 9, 2, 0, 32, 1, 0, 2, 0, 0, 32, 0, 1, 1, 2, 1, 32, 0, 0, 7, 1, 0, 32, 1, 1, 4, 3\n",
       "1, 16, 0, 1, 3, 0, 0, 32, 1, 1, 5, 2, 1, 128, 0, 0, 4, 2, 0, 128, 0, 1, 6, 2, 0, 128, 0, 1, 8, 2, 0, 64, 1, 0, 10, 1, 0, 128, 0, 0, 0, 0\n",
       "1, 8, 0, 0, 1, 0, 1, 8, 1, 0, 5, 2, 0, 32, 1, 1, 6, 2, 1, 64, 0, 0, 5, 0, 1, 16, 0, 1, 4, 0, 1, 32, 1, 1, 8, 0, 0, 512, 0, 1, 2, 0\n",
       "1, 256, 0, 0, 6, 0, 1, 16, 1, 0, 7, 2, 1, 256, 0, 0, 2, 1, 0, 128, 1, 0, 8, 0, 0, 128, 0, 0, 6, 0, 1, 16, 1, 0, 3, 1, 0, 16, 1, 1, 7, 0\n",
       "1, 16, 1, 1, 2, 0, 0, 16, 0, 1, 2, 1, 0, 32, 1, 1, 9, 2, 1, 8, 0, 0, 3, 2, 0, 16, 1, 1, 9, 0, 1, 8, 0, 0, 1, 1, 1, 64, 0, 1, 8, 3\n",
       "1, 64, 1, 1, 10, 1, 1, 128, 0, 0, 9, 1, 0, 16, 0, 1, 10, 2, 1, 256, 0, 0, 10, 2, 1, 8, 0, 0, 2, 0, 1, 256, 0, 1, 10, 0, 1, 32, 1, 0, 0, 2\n",
       "1, 64, 0, 1, 1, 0, 1, 128, 0, 0, 10, 2, 0, 128, 0, 1, 10, 1, 0, 32, 1, 0, 8, 1, 0, 8, 0, 1, 4, 1, 0, 32, 0, 1, 4, 0, 1, 64, 0, 0, 8, 1\n",
       "1, 128, 0, 1, 2, 0, 0, 8, 1, 0, 10, 0, 1, 256, 0, 1, 3, 1, 0, 16, 0, 0, 6, 0, 1, 128, 1, 0, 8, 2, 1, 16, 0, 0, 0, 1, 0, 16, 0, 1, 8, 0\n",
       "1, 16, 1, 1, 5, 2, 0, 8, 1, 0, 10, 2, 1, 128, 1, 0, 6, 1, 0, 16, 1, 1, 9, 0, 0, 256, 1, 1, 7, 1, 0, 32, 0, 1, 8, 0, 1, 32, 0, 1, 7, 1\n",
       "1, 256, 1, 0, 3, 2, 0, 64, 1, 0, 4, 1, 1, 32, 0, 1, 1, 0, 0, 8, 0, 1, 1, 2, 1, 16, 0, 1, 1, 2, 1, 256, 1, 0, 1, 1, 1, 32, 0, 0, 3, 0\n",
       "1, 128, 0, 0, 10, 2, 0, 32, 1, 1, 2, 2, 1, 8, 0, 0, 1, 1, 0, 32, 0, 1, 5, 2, 1, 64, 1, 0, 0, 2, 1, 256, 1, 0, 4, 0, 0, 1024, 1, 1, 5, 3\n",
       "1, 128, 1, 1, 10, 1, 0, 64, 1, 1, 4, 2, 1, 128, 1, 0, 8, 1, 1, 32, 1, 0, 7, 2, 1, 128, 1, 1, 8, 0, 1, 16, 0, 1, 1, 2, 0, 256, 0, 1, 2, 0\n",
       "1, 8, 1, 1, 0, 2, 1, 16, 0, 1, 6, 2, 1, 16, 1, 0, 8, 0, 0, 8, 0, 1, 10, 1, 1, 128, 0, 1, 8, 2, 0, 64, 1, 1, 0, 2, 1, 32, 0, 0, 6, 2\n",
       "1, 64, 0, 1, 4, 1, 1, 64, 1, 0, 7, 2, 0, 8, 0, 1, 0, 0, 1, 16, 0, 1, 7, 2, 1, 128, 0, 0, 0, 2, 1, 64, 1, 1, 3, 1, 1, 1024, 1, 1, 8, 0\n",
       "1, 256, 1, 0, 6, 2, 0, 16, 0, 0, 9, 1, 0, 256, 0, 1, 3, 2, 1, 32, 0, 1, 8, 2, 1, 128, 0, 1, 8, 1, 0, 64, 1, 1, 8, 0, 1, 128, 0, 0, 3, 3\n",
       "1, 8, 0, 0, 4, 1, 1, 16, 0, 1, 10, 1, 0, 32, 1, 1, 9, 1, 0, 256, 0, 0, 8, 1, 1, 16, 0, 1, 9, 2, 0, 16, 1, 0, 4, 1, 0, 64, 1, 0, 7, 1\n",
       "1, 128, 0, 0, 0, 1, 0, 256, 0, 1, 0, 0, 0, 8, 1, 0, 1, 1, 0, 128, 0, 1, 0, 0, 1, 8, 0, 0, 0, 0, 1, 64, 1, 0, 0, 1, 0, 64, 1, 1, 3, 0\n",
       "1, 32, 0, 1, 10, 0, 1, 64, 0, 1, 0, 2, 1, 32, 1, 0, 4, 1, 0, 256, 0, 0, 7, 2, 1, 8, 1, 1, 7, 2, 1, 64, 1, 0, 7, 0, 1, 32, 0, 0, 3, 3\n",
       "1, 256, 1, 0, 6, 1, 0, 8, 0, 1, 3, 1, 1, 16, 1, 1, 8, 2, 0, 128, 1, 0, 6, 1, 0, 256, 1, 0, 3, 1, 0, 32, 0, 1, 0, 0, 0, 512, 0, 1, 5, 0\n",
       "1, 16, 1, 0, 9, 2, 1, 32, 0, 0, 9, 2, 1, 32, 0, 0, 10, 2, 1, 256, 0, 0, 6, 0, 0, 64, 1, 1, 7, 0, 1, 128, 1, 1, 8, 0, 0, 32, 0, 0, 7, 3\n",
       "1, 64, 0, 0, 0, 0, 1, 32, 1, 0, 9, 2, 1, 64, 0, 1, 4, 1, 1, 128, 1, 0, 3, 2, 0, 256, 0, 1, 4, 2, 1, 8, 0, 0, 8, 1, 1, 32, 1, 1, 7, 2\n",
       "1, 256, 0, 1, 8, 2, 1, 128, 1, 0, 3, 0, 0, 256, 1, 1, 8, 2, 0, 256, 0, 1, 5, 2, 0, 256, 0, 1, 3, 2, 0, 256, 1, 0, 4, 1, 1, 256, 0, 0, 5, 1\n",
       "1, 64, 1, 0, 7, 2, 0, 32, 0, 1, 8, 1, 1, 256, 1, 1, 9, 1, 0, 32, 1, 1, 1, 2, 0, 64, 0, 1, 10, 1, 0, 32, 1, 0, 5, 2, 1, 16, 1, 1, 10, 2\n",
       "1, 32, 0, 1, 7, 0, 0, 16, 0, 0, 6, 2, 1, 256, 1, 1, 9, 1, 0, 64, 1, 0, 5, 2, 1, 128, 1, 1, 8, 1, 0, 128, 1, 1, 8, 1, 0, 32, 1, 1, 9, 0\n",
       "1, 64, 0, 0, 1, 0, 1, 16, 0, 0, 0, 0, 1, 128, 0, 0, 2, 1, 0, 256, 1, 1, 9, 0, 1, 64, 0, 0, 3, 2, 1, 16, 0, 0, 4, 0, 0, 128, 0, 1, 9, 2\n",
       "1, 128, 0, 1, 9, 0, 0, 32, 0, 1, 4, 2, 0, 8, 0, 0, 1, 2, 0, 8, 1, 0, 7, 1, 0, 64, 0, 0, 0, 2, 0, 8, 0, 0, 1, 1, 0, 32, 0, 1, 1, 1\n",
       "1, 256, 0, 0, 10, 1, 1, 16, 1, 1, 2, 0, 1, 128, 0, 0, 4, 0, 0, 32, 1, 0, 9, 1, 1, 256, 0, 1, 9, 2, 0, 128, 1, 0, 9, 0, 1, 1024, 1, 1, 0, 3\n",
       "1, 64, 1, 0, 5, 0, 1, 16, 1, 0, 1, 0, 0, 8, 0, 0, 10, 0, 0, 256, 0, 1, 5, 0, 1, 8, 0, 1, 8, 1, 0, 16, 1, 0, 8, 0, 0, 64, 0, 1, 3, 0\n",
       "1, 16, 1, 1, 4, 0, 0, 16, 1, 1, 10, 2, 0, 32, 1, 1, 2, 2, 0, 32, 1, 1, 0, 0, 0, 256, 0, 0, 10, 2, 1, 256, 0, 1, 6, 0, 0, 32, 1, 1, 3, 3\n",
       "1, 64, 0, 1, 0, 2, 1, 256, 1, 1, 0, 2, 0, 128, 0, 0, 10, 0, 1, 32, 1, 1, 1, 2, 0, 8, 1, 0, 2, 1, 1, 128, 1, 0, 3, 0, 0, 16, 0, 0, 2, 0\n",
       "1, 16, 1, 1, 7, 0, 1, 64, 1, 1, 6, 2, 0, 128, 1, 0, 6, 2, 1, 16, 0, 1, 1, 0, 1, 32, 0, 0, 7, 0, 0, 64, 0, 1, 4, 1, 1, 64, 1, 1, 2, 3\n",
       "1, 32, 1, 1, 2, 2, 1, 32, 0, 1, 0, 0, 1, 32, 0, 0, 10, 2, 0, 16, 0, 1, 10, 2, 0, 128, 0, 0, 0, 0, 0, 64, 1, 1, 3, 0, 0, 64, 1, 1, 7, 3\n",
       "1, 64, 0, 0, 9, 1, 1, 128, 0, 0, 8, 1, 1, 128, 0, 1, 4, 0, 0, 256, 0, 1, 7, 2, 1, 8, 0, 1, 0, 1, 0, 16, 1, 1, 1, 1, 1, 32, 1, 1, 7, 1\n",
       "1, 256, 0, 0, 2, 2, 0, 16, 0, 0, 9, 2, 1, 8, 0, 1, 9, 2, 1, 8, 0, 1, 7, 1, 0, 8, 1, 0, 9, 0, 1, 32, 1, 0, 1, 0, 1, 32, 0, 0, 8, 0\n",
       "1, 64, 0, 1, 2, 0, 1, 32, 0, 0, 5, 0, 0, 128, 1, 0, 1, 0, 0, 256, 0, 1, 6, 0, 0, 256, 1, 0, 2, 1, 0, 256, 1, 1, 6, 1, 0, 128, 0, 1, 7, 0\n",
       "1, 32, 1, 0, 7, 0, 0, 32, 0, 0, 3, 2, 0, 32, 0, 0, 6, 1, 0, 128, 1, 0, 1, 2, 0, 8, 0, 0, 10, 2, 0, 32, 0, 1, 1, 1, 0, 256, 1, 1, 10, 1\n",
       "1, 256, 0, 1, 4, 0, 0, 32, 1, 1, 2, 0, 0, 8, 1, 1, 3, 0, 1, 256, 0, 1, 4, 0, 0, 256, 1, 1, 7, 0, 1, 128, 1, 0, 3, 1, 0, 16, 1, 1, 4, 2\n",
       "1, 16, 1, 1, 7, 1, 1, 128, 0, 1, 4, 0, 1, 32, 0, 1, 10, 0, 1, 8, 0, 0, 1, 2, 1, 16, 1, 0, 6, 1, 1, 8, 0, 1, 8, 1, 0, 16, 1, 1, 5, 3\n",
       "1, 16, 0, 0, 9, 2, 0, 64, 0, 1, 7, 0, 1, 8, 0, 0, 0, 1, 0, 64, 0, 0, 5, 2, 0, 64, 1, 0, 7, 0, 1, 256, 0, 0, 7, 0, 1, 128, 0, 0, 6, 0\n",
       "1, 64, 1, 1, 7, 2, 0, 128, 0, 1, 8, 1, 1, 16, 1, 0, 4, 1, 1, 8, 0, 1, 2, 0, 1, 128, 1, 1, 8, 2, 1, 128, 0, 0, 8, 2, 0, 1024, 1, 1, 0, 0\n",
       "1, 128, 1, 1, 3, 2, 1, 32, 0, 1, 0, 1, 0, 16, 1, 0, 10, 2, 0, 16, 0, 1, 3, 1, 1, 64, 1, 1, 7, 1, 1, 256, 1, 1, 2, 2, 0, 32, 1, 0, 1, 2\n",
       "1, 32, 0, 0, 10, 2, 1, 8, 1, 1, 0, 0, 0, 64, 0, 1, 3, 1, 0, 128, 0, 0, 3, 0, 1, 128, 0, 0, 1, 2, 1, 32, 0, 1, 1, 2, 0, 1024, 0, 1, 6, 3\n",
       "1, 64, 0, 0, 0, 2, 0, 8, 1, 1, 2, 0, 0, 8, 0, 0, 4, 2, 1, 16, 0, 1, 2, 0, 1, 8, 1, 1, 10, 1, 1, 256, 1, 0, 9, 2, 0, 512, 1, 1, 8, 1\n",
       "1, 256, 1, 0, 0, 0, 0, 32, 0, 0, 3, 0, 1, 256, 0, 0, 7, 2, 1, 8, 1, 0, 5, 1, 0, 64, 1, 0, 5, 0, 1, 128, 0, 0, 3, 2, 1, 128, 1, 0, 6, 2\n",
       "1, 64, 1, 0, 6, 2, 1, 8, 1, 0, 9, 1, 0, 16, 1, 1, 0, 0, 1, 32, 0, 0, 0, 0, 1, 256, 1, 0, 1, 2, 0, 64, 0, 0, 10, 0, 1, 256, 0, 0, 1, 3\n",
       "1, 16, 0, 0, 3, 2, 0, 32, 0, 0, 2, 0, 0, 64, 1, 0, 4, 0, 0, 32, 1, 0, 1, 1, 0, 64, 1, 0, 5, 1, 1, 256, 0, 0, 10, 2, 1, 1024, 1, 0, 1, 3\n",
       "1, 64, 0, 1, 1, 0, 1, 64, 0, 0, 5, 2, 1, 32, 1, 0, 4, 1, 1, 64, 1, 1, 2, 1, 0, 32, 1, 1, 6, 2, 1, 128, 0, 1, 1, 0, 0, 256, 1, 0, 0, 1\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Genome encoding and metric data stored at /dbfs/FileStore/runs/uniform_xover_25chcFri_Nov_27_20-51-59_2020 \n\n1, 64, 0, 0, 6, 1, 1, 256, 1, 1, 5, 0, 1, 128, 0, 1, 8, 0, 0, 64, 0, 0, 1, 1, 1, 32, 1, 0, 2, 1, 0, 128, 1, 1, 10, 2, 0, 16, 0, 1, 9, 3\n1, 8, 0, 0, 10, 0, 1, 256, 0, 1, 4, 1, 1, 8, 0, 1, 2, 1, 0, 256, 1, 0, 4, 1, 1, 8, 1, 1, 6, 1, 1, 32, 0, 0, 4, 0, 0, 64, 0, 1, 10, 2\n1, 128, 0, 1, 3, 1, 0, 32, 1, 0, 6, 1, 1, 256, 0, 1, 2, 1, 1, 64, 1, 0, 4, 0, 0, 128, 1, 1, 10, 0, 0, 16, 0, 0, 1, 2, 0, 128, 0, 1, 4, 1\n1, 16, 0, 0, 3, 0, 0, 16, 1, 0, 5, 1, 1, 128, 0, 1, 9, 2, 0, 32, 1, 0, 2, 0, 0, 32, 0, 1, 1, 2, 1, 32, 0, 0, 7, 1, 0, 32, 1, 1, 4, 3\n1, 16, 0, 1, 3, 0, 0, 32, 1, 1, 5, 2, 1, 128, 0, 0, 4, 2, 0, 128, 0, 1, 6, 2, 0, 128, 0, 1, 8, 2, 0, 64, 1, 0, 10, 1, 0, 128, 0, 0, 0, 0\n1, 8, 0, 0, 1, 0, 1, 8, 1, 0, 5, 2, 0, 32, 1, 1, 6, 2, 1, 64, 0, 0, 5, 0, 1, 16, 0, 1, 4, 0, 1, 32, 1, 1, 8, 0, 0, 512, 0, 1, 2, 0\n1, 256, 0, 0, 6, 0, 1, 16, 1, 0, 7, 2, 1, 256, 0, 0, 2, 1, 0, 128, 1, 0, 8, 0, 0, 128, 0, 0, 6, 0, 1, 16, 1, 0, 3, 1, 0, 16, 1, 1, 7, 0\n1, 16, 1, 1, 2, 0, 0, 16, 0, 1, 2, 1, 0, 32, 1, 1, 9, 2, 1, 8, 0, 0, 3, 2, 0, 16, 1, 1, 9, 0, 1, 8, 0, 0, 1, 1, 1, 64, 0, 1, 8, 3\n1, 64, 1, 1, 10, 1, 1, 128, 0, 0, 9, 1, 0, 16, 0, 1, 10, 2, 1, 256, 0, 0, 10, 2, 1, 8, 0, 0, 2, 0, 1, 256, 0, 1, 10, 0, 1, 32, 1, 0, 0, 2\n1, 64, 0, 1, 1, 0, 1, 128, 0, 0, 10, 2, 0, 128, 0, 1, 10, 1, 0, 32, 1, 0, 8, 1, 0, 8, 0, 1, 4, 1, 0, 32, 0, 1, 4, 0, 1, 64, 0, 0, 8, 1\n1, 128, 0, 1, 2, 0, 0, 8, 1, 0, 10, 0, 1, 256, 0, 1, 3, 1, 0, 16, 0, 0, 6, 0, 1, 128, 1, 0, 8, 2, 1, 16, 0, 0, 0, 1, 0, 16, 0, 1, 8, 0\n1, 16, 1, 1, 5, 2, 0, 8, 1, 0, 10, 2, 1, 128, 1, 0, 6, 1, 0, 16, 1, 1, 9, 0, 0, 256, 1, 1, 7, 1, 0, 32, 0, 1, 8, 0, 1, 32, 0, 1, 7, 1\n1, 256, 1, 0, 3, 2, 0, 64, 1, 0, 4, 1, 1, 32, 0, 1, 1, 0, 0, 8, 0, 1, 1, 2, 1, 16, 0, 1, 1, 2, 1, 256, 1, 0, 1, 1, 1, 32, 0, 0, 3, 0\n1, 128, 0, 0, 10, 2, 0, 32, 1, 1, 2, 2, 1, 8, 0, 0, 1, 1, 0, 32, 0, 1, 5, 2, 1, 64, 1, 0, 0, 2, 1, 256, 1, 0, 4, 0, 0, 1024, 1, 1, 5, 3\n1, 128, 1, 1, 10, 1, 0, 64, 1, 1, 4, 2, 1, 128, 1, 0, 8, 1, 1, 32, 1, 0, 7, 2, 1, 128, 1, 1, 8, 0, 1, 16, 0, 1, 1, 2, 0, 256, 0, 1, 2, 0\n1, 8, 1, 1, 0, 2, 1, 16, 0, 1, 6, 2, 1, 16, 1, 0, 8, 0, 0, 8, 0, 1, 10, 1, 1, 128, 0, 1, 8, 2, 0, 64, 1, 1, 0, 2, 1, 32, 0, 0, 6, 2\n1, 64, 0, 1, 4, 1, 1, 64, 1, 0, 7, 2, 0, 8, 0, 1, 0, 0, 1, 16, 0, 1, 7, 2, 1, 128, 0, 0, 0, 2, 1, 64, 1, 1, 3, 1, 1, 1024, 1, 1, 8, 0\n1, 256, 1, 0, 6, 2, 0, 16, 0, 0, 9, 1, 0, 256, 0, 1, 3, 2, 1, 32, 0, 1, 8, 2, 1, 128, 0, 1, 8, 1, 0, 64, 1, 1, 8, 0, 1, 128, 0, 0, 3, 3\n1, 8, 0, 0, 4, 1, 1, 16, 0, 1, 10, 1, 0, 32, 1, 1, 9, 1, 0, 256, 0, 0, 8, 1, 1, 16, 0, 1, 9, 2, 0, 16, 1, 0, 4, 1, 0, 64, 1, 0, 7, 1\n1, 128, 0, 0, 0, 1, 0, 256, 0, 1, 0, 0, 0, 8, 1, 0, 1, 1, 0, 128, 0, 1, 0, 0, 1, 8, 0, 0, 0, 0, 1, 64, 1, 0, 0, 1, 0, 64, 1, 1, 3, 0\n1, 32, 0, 1, 10, 0, 1, 64, 0, 1, 0, 2, 1, 32, 1, 0, 4, 1, 0, 256, 0, 0, 7, 2, 1, 8, 1, 1, 7, 2, 1, 64, 1, 0, 7, 0, 1, 32, 0, 0, 3, 3\n1, 256, 1, 0, 6, 1, 0, 8, 0, 1, 3, 1, 1, 16, 1, 1, 8, 2, 0, 128, 1, 0, 6, 1, 0, 256, 1, 0, 3, 1, 0, 32, 0, 1, 0, 0, 0, 512, 0, 1, 5, 0\n1, 16, 1, 0, 9, 2, 1, 32, 0, 0, 9, 2, 1, 32, 0, 0, 10, 2, 1, 256, 0, 0, 6, 0, 0, 64, 1, 1, 7, 0, 1, 128, 1, 1, 8, 0, 0, 32, 0, 0, 7, 3\n1, 64, 0, 0, 0, 0, 1, 32, 1, 0, 9, 2, 1, 64, 0, 1, 4, 1, 1, 128, 1, 0, 3, 2, 0, 256, 0, 1, 4, 2, 1, 8, 0, 0, 8, 1, 1, 32, 1, 1, 7, 2\n1, 256, 0, 1, 8, 2, 1, 128, 1, 0, 3, 0, 0, 256, 1, 1, 8, 2, 0, 256, 0, 1, 5, 2, 0, 256, 0, 1, 3, 2, 0, 256, 1, 0, 4, 1, 1, 256, 0, 0, 5, 1\n1, 64, 1, 0, 7, 2, 0, 32, 0, 1, 8, 1, 1, 256, 1, 1, 9, 1, 0, 32, 1, 1, 1, 2, 0, 64, 0, 1, 10, 1, 0, 32, 1, 0, 5, 2, 1, 16, 1, 1, 10, 2\n1, 32, 0, 1, 7, 0, 0, 16, 0, 0, 6, 2, 1, 256, 1, 1, 9, 1, 0, 64, 1, 0, 5, 2, 1, 128, 1, 1, 8, 1, 0, 128, 1, 1, 8, 1, 0, 32, 1, 1, 9, 0\n1, 64, 0, 0, 1, 0, 1, 16, 0, 0, 0, 0, 1, 128, 0, 0, 2, 1, 0, 256, 1, 1, 9, 0, 1, 64, 0, 0, 3, 2, 1, 16, 0, 0, 4, 0, 0, 128, 0, 1, 9, 2\n1, 128, 0, 1, 9, 0, 0, 32, 0, 1, 4, 2, 0, 8, 0, 0, 1, 2, 0, 8, 1, 0, 7, 1, 0, 64, 0, 0, 0, 2, 0, 8, 0, 0, 1, 1, 0, 32, 0, 1, 1, 1\n1, 256, 0, 0, 10, 1, 1, 16, 1, 1, 2, 0, 1, 128, 0, 0, 4, 0, 0, 32, 1, 0, 9, 1, 1, 256, 0, 1, 9, 2, 0, 128, 1, 0, 9, 0, 1, 1024, 1, 1, 0, 3\n1, 64, 1, 0, 5, 0, 1, 16, 1, 0, 1, 0, 0, 8, 0, 0, 10, 0, 0, 256, 0, 1, 5, 0, 1, 8, 0, 1, 8, 1, 0, 16, 1, 0, 8, 0, 0, 64, 0, 1, 3, 0\n1, 16, 1, 1, 4, 0, 0, 16, 1, 1, 10, 2, 0, 32, 1, 1, 2, 2, 0, 32, 1, 1, 0, 0, 0, 256, 0, 0, 10, 2, 1, 256, 0, 1, 6, 0, 0, 32, 1, 1, 3, 3\n1, 64, 0, 1, 0, 2, 1, 256, 1, 1, 0, 2, 0, 128, 0, 0, 10, 0, 1, 32, 1, 1, 1, 2, 0, 8, 1, 0, 2, 1, 1, 128, 1, 0, 3, 0, 0, 16, 0, 0, 2, 0\n1, 16, 1, 1, 7, 0, 1, 64, 1, 1, 6, 2, 0, 128, 1, 0, 6, 2, 1, 16, 0, 1, 1, 0, 1, 32, 0, 0, 7, 0, 0, 64, 0, 1, 4, 1, 1, 64, 1, 1, 2, 3\n1, 32, 1, 1, 2, 2, 1, 32, 0, 1, 0, 0, 1, 32, 0, 0, 10, 2, 0, 16, 0, 1, 10, 2, 0, 128, 0, 0, 0, 0, 0, 64, 1, 1, 3, 0, 0, 64, 1, 1, 7, 3\n1, 64, 0, 0, 9, 1, 1, 128, 0, 0, 8, 1, 1, 128, 0, 1, 4, 0, 0, 256, 0, 1, 7, 2, 1, 8, 0, 1, 0, 1, 0, 16, 1, 1, 1, 1, 1, 32, 1, 1, 7, 1\n1, 256, 0, 0, 2, 2, 0, 16, 0, 0, 9, 2, 1, 8, 0, 1, 9, 2, 1, 8, 0, 1, 7, 1, 0, 8, 1, 0, 9, 0, 1, 32, 1, 0, 1, 0, 1, 32, 0, 0, 8, 0\n1, 64, 0, 1, 2, 0, 1, 32, 0, 0, 5, 0, 0, 128, 1, 0, 1, 0, 0, 256, 0, 1, 6, 0, 0, 256, 1, 0, 2, 1, 0, 256, 1, 1, 6, 1, 0, 128, 0, 1, 7, 0\n1, 32, 1, 0, 7, 0, 0, 32, 0, 0, 3, 2, 0, 32, 0, 0, 6, 1, 0, 128, 1, 0, 1, 2, 0, 8, 0, 0, 10, 2, 0, 32, 0, 1, 1, 1, 0, 256, 1, 1, 10, 1\n1, 256, 0, 1, 4, 0, 0, 32, 1, 1, 2, 0, 0, 8, 1, 1, 3, 0, 1, 256, 0, 1, 4, 0, 0, 256, 1, 1, 7, 0, 1, 128, 1, 0, 3, 1, 0, 16, 1, 1, 4, 2\n1, 16, 1, 1, 7, 1, 1, 128, 0, 1, 4, 0, 1, 32, 0, 1, 10, 0, 1, 8, 0, 0, 1, 2, 1, 16, 1, 0, 6, 1, 1, 8, 0, 1, 8, 1, 0, 16, 1, 1, 5, 3\n1, 16, 0, 0, 9, 2, 0, 64, 0, 1, 7, 0, 1, 8, 0, 0, 0, 1, 0, 64, 0, 0, 5, 2, 0, 64, 1, 0, 7, 0, 1, 256, 0, 0, 7, 0, 1, 128, 0, 0, 6, 0\n1, 64, 1, 1, 7, 2, 0, 128, 0, 1, 8, 1, 1, 16, 1, 0, 4, 1, 1, 8, 0, 1, 2, 0, 1, 128, 1, 1, 8, 2, 1, 128, 0, 0, 8, 2, 0, 1024, 1, 1, 0, 0\n1, 128, 1, 1, 3, 2, 1, 32, 0, 1, 0, 1, 0, 16, 1, 0, 10, 2, 0, 16, 0, 1, 3, 1, 1, 64, 1, 1, 7, 1, 1, 256, 1, 1, 2, 2, 0, 32, 1, 0, 1, 2\n1, 32, 0, 0, 10, 2, 1, 8, 1, 1, 0, 0, 0, 64, 0, 1, 3, 1, 0, 128, 0, 0, 3, 0, 1, 128, 0, 0, 1, 2, 1, 32, 0, 1, 1, 2, 0, 1024, 0, 1, 6, 3\n1, 64, 0, 0, 0, 2, 0, 8, 1, 1, 2, 0, 0, 8, 0, 0, 4, 2, 1, 16, 0, 1, 2, 0, 1, 8, 1, 1, 10, 1, 1, 256, 1, 0, 9, 2, 0, 512, 1, 1, 8, 1\n1, 256, 1, 0, 0, 0, 0, 32, 0, 0, 3, 0, 1, 256, 0, 0, 7, 2, 1, 8, 1, 0, 5, 1, 0, 64, 1, 0, 5, 0, 1, 128, 0, 0, 3, 2, 1, 128, 1, 0, 6, 2\n1, 64, 1, 0, 6, 2, 1, 8, 1, 0, 9, 1, 0, 16, 1, 1, 0, 0, 1, 32, 0, 0, 0, 0, 1, 256, 1, 0, 1, 2, 0, 64, 0, 0, 10, 0, 1, 256, 0, 0, 1, 3\n1, 16, 0, 0, 3, 2, 0, 32, 0, 0, 2, 0, 0, 64, 1, 0, 4, 0, 0, 32, 1, 0, 1, 1, 0, 64, 1, 0, 5, 1, 1, 256, 0, 0, 10, 2, 1, 1024, 1, 0, 1, 3\n1, 64, 0, 1, 1, 0, 1, 64, 0, 0, 5, 2, 1, 32, 1, 0, 4, 1, 1, 64, 1, 1, 2, 1, 0, 32, 1, 1, 6, 2, 1, 128, 0, 1, 1, 0, 0, 256, 1, 0, 0, 1\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=20,\n",
    "                  pop_size=10,\n",
    "                  epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d5697a6-58c6-4a4b-92b8-ebf1a25e095f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=20,\n",
    "                  pop_size=10,\n",
    "                  epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd2105f9-7d1e-4598-a6f1-8292a86788d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=20,\n",
    "                  pop_size=10,\n",
    "                  epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89aa284a-4854-4c57-b7d0-212d83da6c7d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=30,\n",
    "                  pop_size=50,\n",
    "                  epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e50b6ff6-f0b0-47c9-998e-6e001fa3a237",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=30,\n",
    "                  pop_size=50,\n",
    "                  epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "26b99dd5-dec3-4884-bf71-ac93b0b07d67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=30,\n",
    "                  pop_size=50,\n",
    "                  epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6901d465-1c9d-494b-8ab9-819b91b5f213",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=30,\n",
    "                  pop_size=50,\n",
    "                  epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "adf6244b-da70-4583-99df-c415659cd07e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=30,\n",
    "                  pop_size=50,\n",
    "                  epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2586db07-30b8-4957-8f81-6a7d3e99bd61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=30,\n",
    "                  pop_size=50,\n",
    "                  epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "508f56c0-119b-47bf-847d-4ab252e79a7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=30,\n",
    "                  pop_size=50,\n",
    "                  epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34885300-99b4-4406-b46b-f4401779c4ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=30,\n",
    "                  pop_size=50,\n",
    "                  epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "290eeb5a-b21b-4e7e-9575-fe1b942a2759",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "devol = DEvol(genome_handler)\n",
    "devol.run(dataset=dataset,\n",
    "                  num_generations=30,\n",
    "                  pop_size=50,\n",
    "                  epochs=5)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "uniform_xover_chc",
   "notebookOrigID": 4275497423461445,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
